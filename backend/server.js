import 'dotenv/config';
import express from 'express';
import cors from 'cors';
import rateLimit from 'express-rate-limit';
import { z } from 'zod';

const app = express();

// ============================================
// Configuration
// ============================================
const allowedOrigins = (process.env.CORS_ORIGIN || '*')
  .split(',')
  .map(s => s.trim());

const PORT = process.env.PORT || 3001;
const PROVIDER = process.env.PROVIDER || 'gemini';

// ============================================
// Middleware
// ============================================
app.use(cors({
  origin: (origin, cb) => {
    if (!origin || allowedOrigins.includes('*') || allowedOrigins.includes(origin)) {
      return cb(null, true);
    }
    return cb(new Error(`CORS blocked for origin: ${origin}`));
  },
  credentials: false
}));

app.use(express.json({ limit: '1mb' }));

// Rate limiting
const limiter = rateLimit({ 
  windowMs: 60 * 1000, 
  max: 60,
  message: { error: 'Too many requests, please try again later.' }
});
app.use('/api/', limiter);

// Request logging
app.use((req, res, next) => {
  const timestamp = new Date().toISOString();
  console.log(`[${timestamp}] ${req.method} ${req.path}`);
  next();
});

// ============================================
// Validation Schema
// ============================================
const InputSchema = z.object({
  messages: z.array(
    z.object({
      role: z.enum(['user', 'assistant', 'system']),
      content: z.string().min(1).max(4000)
    })
  ).min(1).max(20),
});

// ============================================
// Optimized System Prompt - Fast & Detailed
// ============================================
const SYSTEM_PROMPT = `Báº¡n lÃ  AI chuyÃªn tÆ° váº¥n linh kiá»‡n PC vÃ  build mÃ¡y tÃ­nh. Tráº£ lá»i NGáº®N Gá»ŒN nhÆ°ng Äáº¦Y Äá»¦ THÃ”NG TIN.

ğŸ“Š GIÃ LINH KIá»†N 2025 (VNÄ):

**CPU Gaming:**
â€¢ i5-14400F: 5tr (10 nhÃ¢n, 1080p-1440p tá»‘t)
â€¢ i5-13600K: 6.5tr (14 nhÃ¢n, gaming + stream)
â€¢ i7-13700F: 8tr (16 nhÃ¢n, Ä‘a nhiá»‡m máº¡nh)
â€¢ i7-14700K: 10tr (20 nhÃ¢n, gaming + workstation)
â€¢ Ryzen 5 7600: 5.5tr (6 nhÃ¢n, IPC cao)
â€¢ Ryzen 7 7800X3D: 10tr (8 nhÃ¢n, gaming tá»‘t nháº¥t)

**GPU Gaming:**
â€¢ RTX 4060 8GB: 8-9tr â†’ 1080p Ultra 60+ fps
â€¢ RTX 4060 Ti 8GB: 10-11tr â†’ 1440p High-Ultra 60+ fps
â€¢ RX 7600 8GB: 7-8tr â†’ 1080p Ultra (giÃ¡ tá»‘t)
â€¢ RTX 4070 12GB: 14-15tr â†’ 1440p Ultra, 4K Medium 60fps
â€¢ RTX 4070 Super 12GB: 16-17tr â†’ 1440p-4K Ultra
â€¢ RTX 4070 Ti Super 16GB: 21-22tr â†’ 4K Ultra 60+ fps
â€¢ RTX 4080 Super 16GB: 28-30tr â†’ 4K Ultra 100+ fps

**Main + RAM:**
â€¢ B760 (Intel 12-14 gen): 3tr + DDR4 hoáº·c DDR5
â€¢ B650 (AMD Ryzen 7000): 3.5tr + DDR5 báº¯t buá»™c
â€¢ DDR4 16GB (2x8) 3200MHz: 1.3tr
â€¢ DDR5 16GB (2x8) 5600MHz: 2tr
â€¢ DDR5 32GB (2x16) 6000MHz: 4tr

**LÆ°u trá»¯ + PSU:**
â€¢ SSD NVMe Gen4 500GB: 1tr
â€¢ SSD NVMe Gen4 1TB: 1.7tr
â€¢ SSD NVMe Gen4 2TB: 3tr
â€¢ PSU 650W 80+ Gold: 1.7tr
â€¢ PSU 750W 80+ Gold: 2.2tr
â€¢ PSU 850W 80+ Gold: 2.7tr

**Case + Táº£n nhiá»‡t:**
â€¢ Case ATX airflow tá»‘t: 1.5-2tr
â€¢ Táº£n nhiá»‡t khÃ­ tower: 500k-1tr
â€¢ AIO 240mm: 2-2.5tr

**MÃ n hÃ¬nh Gaming:**
â€¢ 1080p 144Hz IPS: 3-4tr
â€¢ 1440p 165Hz IPS: 5-7tr
â€¢ 4K 144Hz IPS: 10-15tr

âš¡ NGUYÃŠN Táº®C TÆ¯ Váº¤N:

1. **Há»i ngáº¯n gá»n:** NgÃ¢n sÃ¡ch? Má»¥c Ä‘Ã­ch (game gÃ¬/work)? CÃ³ mÃ n hÃ¬nh chÆ°a?

2. **Äá»€ XUáº¤T CHUáº¨N (3-5 dÃ²ng):**
ğŸ’» Build Gaming [Äá»™ phÃ¢n giáº£i] - [GiÃ¡]tr
â€¢ CPU: [TÃªn] ([GiÃ¡]tr)
â€¢ GPU: [TÃªn] ([GiÃ¡]tr) â†’ [Hiá»‡u nÄƒng]
â€¢ Main: [TÃªn] + RAM: [Dung lÆ°á»£ng] ([GiÃ¡]tr)
â€¢ SSD: [Dung lÆ°á»£ng] ([GiÃ¡]tr) + PSU: [CÃ´ng suáº¥t] ([GiÃ¡]tr) + Case ([GiÃ¡]tr)
ğŸ’° Tá»•ng: ~[X]tr
âš ï¸ [LÆ°u Ã½ tÆ°Æ¡ng thÃ­ch quan trá»ng]
Cáº§n gÃ¬ thÃªm?

3. **Cáº¢NH BÃO QUAN TRá»ŒNG:**
- Intel 12-14 gen â†” Main B760/Z790
- AMD Ryzen 7000 â†” Main B650 + DDR5 báº¯t buá»™c
- RTX 4070+ cáº§n PSU 750W+
- GPU dÃ i check case (thÆ°á»ng 300-330mm)

4. **SO SÃNH (náº¿u Ä‘Æ°á»£c há»i):**
- NÃªu Ä‘iá»ƒm máº¡nh/yáº¿u tá»«ng sáº£n pháº©m
- ÄÆ°a khuyáº¿n nghá»‹ rÃµ rÃ ng

5. **LUÃ”N:** < 150 tá»«, emoji phÃ¹ há»£p, káº¿t thÃºc báº±ng cÃ¢u há»i ngáº¯n

VÃ Dá»¤:
User: "Build gaming 20tr"
Bot: "ğŸ’» **Build Gaming 1080p - 20tr**
â€¢ CPU: i5-14400F (5tr) + Main B760 + RAM DDR5 16GB (5tr)
â€¢ GPU: RTX 4060 8GB (8.5tr) â†’ 1080p Ultra 80+ fps
â€¢ SSD 1TB (1.7tr) + PSU 650W (1.7tr) + Case (1.5tr)
ğŸ’° Tá»•ng: ~23.4tr
âš ï¸ Náº¿u ngÃ¢n sÃ¡ch gáº¯t â†’ dÃ¹ng RX 7600 (7tr) thay RTX 4060
Báº¡n Ä‘Ã£ cÃ³ mÃ n hÃ¬nh chÆ°a?"

TRá»ŒNG TÃ‚M: Nhanh, rÃµ rÃ ng, thá»±c táº¿, giÃºp khÃ¡ch quyáº¿t Ä‘á»‹nh dá»… dÃ ng!`;

app.post('/api/chat', async (req, res) => {
  const parsed = InputSchema.safeParse(req.body);
  if (!parsed.success) {
    console.log('[ERROR] Validation failed:', parsed.error);
    return res.status(400).json({ error: 'Invalid input', details: parsed.error.flatten() });
  }

  const { messages } = parsed.data;
  console.log('[DEBUG] Received messages:', messages.length);

  try {
    if (PROVIDER === 'gemini') {
      const { GoogleGenerativeAI } = await import('@google/generative-ai');
      const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
      const modelName = process.env.GEMINI_MODEL || 'gemini-1.5-flash';
      
      console.log('[DEBUG] Using model:', modelName);
      
      // Optimized generation config for faster responses
      const model = genAI.getGenerativeModel({ 
        model: modelName, 
        systemInstruction: SYSTEM_PROMPT + "\n\nIMPORTANT: Respond directly without extended thinking. Be concise.",
        generationConfig: {
          temperature: 1.0,
          topK: 40,
          topP: 0.95,
          maxOutputTokens: 2048,
          responseMimeType: "text/plain",
        }
      });

      // Get only user messages (ignore system and assistant for simplicity)
      const userMessages = messages.filter(m => m.role === 'user');
      
      if (userMessages.length === 0) {
        return res.status(400).json({ error: 'No user message found' });
      }
      
      // Get last user message
      const lastUserMessage = userMessages[userMessages.length - 1].content;
      console.log('[DEBUG] Last user message:', lastUserMessage.substring(0, 50));
      
      // For first message or simple conversation, just send the message directly
      if (userMessages.length === 1) {
        const result = await model.generateContent(lastUserMessage);
        console.log('[DEBUG] Result candidates:', result.response.candidates?.length);
        
        // Check if response was blocked
        if (result.response.promptFeedback?.blockReason) {
          console.log('[ERROR] Content blocked:', result.response.promptFeedback.blockReason);
          return res.json({ reply: 'Xin lá»—i, ná»™i dung bá»‹ cháº·n bá»Ÿi AI. Vui lÃ²ng thá»­ cÃ¢u há»i khÃ¡c.', provider: 'gemini' });
        }
        
        // Extract text safely
        let text = '';
        try {
          text = result.response.text();
        } catch (e) {
          console.log('[ERROR] text() method failed:', e.message);
          // Fallback: extract from candidates
          if (result.response.candidates && result.response.candidates[0]) {
            const parts = result.response.candidates[0].content.parts;
            text = parts.map(p => p.text).join('');
          }
        }
        
        console.log('[DEBUG] Response length:', text.length);
        console.log('[DEBUG] Response text:', text.substring(0, 100));
        
        if (!text || text.length === 0) {
          console.log('[ERROR] Empty response despite successful API call');
          console.log('[DEBUG] Full response object:', JSON.stringify(result.response, null, 2));
          return res.json({ reply: 'Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i. Vui lÃ²ng thá»­ láº¡i.', provider: 'gemini' });
        }
        
        return res.json({ reply: text, provider: 'gemini' });
      }
      
      // For multi-turn conversation, build history properly
      const history = [];
      for (let i = 0; i < messages.length - 1; i++) {
        const msg = messages[i];
        if (msg.role === 'user') {
          history.push({ role: 'user', parts: [{ text: msg.content }] });
        } else if (msg.role === 'assistant') {
          history.push({ role: 'model', parts: [{ text: msg.content }] });
        }
      }
      
      // Ensure history starts with user
      if (history.length > 0 && history[0].role !== 'user') {
        history.shift();
      }
      
      console.log('[DEBUG] History length:', history.length);
      
      const chat = model.startChat({ history });
      const result = await chat.sendMessage(lastUserMessage);
      
      console.log('[DEBUG] Result candidates:', result.response.candidates?.length);
      
      // Check if response was blocked
      if (result.response.promptFeedback?.blockReason) {
        console.log('[ERROR] Content blocked:', result.response.promptFeedback.blockReason);
        return res.json({ reply: 'Xin lá»—i, ná»™i dung bá»‹ cháº·n bá»Ÿi AI. Vui lÃ²ng thá»­ cÃ¢u há»i khÃ¡c.', provider: 'gemini' });
      }
      
      // Extract text safely
      let text = '';
      try {
        text = result.response.text();
      } catch (e) {
        console.log('[ERROR] text() method failed:', e.message);
        // Fallback: extract from candidates
        if (result.response.candidates && result.response.candidates[0]) {
          const parts = result.response.candidates[0].content.parts;
          text = parts.map(p => p.text).join('');
        }
      }
      
      console.log('[DEBUG] Response length:', text.length);
      console.log('[DEBUG] Response text:', text.substring(0, 100));
      
      if (!text || text.length === 0) {
        console.log('[ERROR] Empty response despite successful API call');
        console.log('[DEBUG] Full response object:', JSON.stringify(result.response, null, 2));
        return res.json({ reply: 'Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i. Vui lÃ²ng thá»­ láº¡i.', provider: 'gemini' });
      }
      
      return res.json({ reply: text, provider: 'gemini' });
    }

    if (PROVIDER === 'cloudflare') {
      const accountId = process.env.CF_ACCOUNT_ID;
      const aiToken = process.env.CF_API_TOKEN;
      const cfModel = process.env.CF_MODEL || '@cf/meta/llama-3-8b-instruct';
      if (!accountId || !aiToken) throw new Error('Missing CF_ACCOUNT_ID or CF_API_TOKEN');

      const url = `https://api.cloudflare.com/client/v4/accounts/${accountId}/ai/run/${cfModel}`;
      
      // Only keep last 8 messages for faster processing
      const recentMessages = messages.slice(-9); // system + 8 messages
      const cfMessages = [ { role: 'system', content: SYSTEM_PROMPT }, ...recentMessages.filter(m => m.role !== 'system') ];

      const r = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${aiToken}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ 
          messages: cfMessages,
          max_tokens: 400  // Limit for faster responses
        })
      });

      const data = await r.json();
      if (!data.success) {
        const errMsg = data.errors?.[0]?.message || 'Cloudflare AI error';
        throw new Error(errMsg);
      }

      const text = data.result?.response || data.result?.output_text || data.result?.message?.content || JSON.stringify(data.result);
      return res.json({ reply: text, provider: 'cloudflare' });
    }

    return res.status(400).json({ error: 'Unsupported provider' });
  } catch (err) {
    console.error(`[ERROR] Chat API:`, err.message);
    console.error(`[ERROR] Stack:`, err.stack);
    return res.status(500).json({ error: err.message || 'Server error', details: err.toString() });
  }
});

// ============================================
// Health Check
// ============================================
app.get('/health', (_req, res) => {
  res.json({ 
    ok: true, 
    provider: PROVIDER,
    timestamp: new Date().toISOString()
  });
});

// ============================================
// Stats endpoint (optional)
// ============================================
let requestCount = 0;
app.get('/api/stats', (_req, res) => {
  requestCount++;
  res.json({
    requests: requestCount,
    uptime: process.uptime(),
    provider: PROVIDER
  });
});

// ============================================
// Error handling
// ============================================
app.use((err, req, res, next) => {
  console.error('[ERROR]', err);
  res.status(500).json({ error: 'Internal server error' });
});

// ============================================
// Start Server
// ============================================
app.listen(PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš€ PC Parts Chatbot Backend                  â•‘
â•‘  ğŸ“¡ Server: http://localhost:${PORT}           â•‘
â•‘  ğŸ¤– Provider: ${PROVIDER.toUpperCase()}                      â•‘
â•‘  âš¡ Status: Ready                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);
});
